


1. Create user and enable console

with this permesion "AdministratorAccess"



2. Create Role For EKS:
- Use case "EKS"

with permission:
AmazonEKSClusterPolicy
AmazonEKSServicePolicy

AmazonEC2FullAccess
AmazonEC2ContainerServiceAutoscaleRole

3. Go To create a "EKS-Cluster":


4. Install aws cli :


[ec2-user@ip-172-31-53-211 ~]$ curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 54.8M  100 54.8M    0     0  97.7M      0 --:--:-- --:--:-- --:--:-- 97.6M


[ec2-user@ip-172-31-53-211 ~]$ unzip awscliv2.zip
Archive:  awscliv2.zip
   creating: aws/
   creating: aws/dist/


[ec2-user@ip-172-31-53-211 ~]$ sudo ./aws/install
You can now run: /usr/local/bin/aws --version

[ec2-user@ip-172-31-53-211 ~]$ aws --version
aws-cli/2.11.6 Python/3.11.2 Linux/5.10.173-154.642.amzn2.x86_64 exe/x86_64.amzn.2 prompt/off




5. Install kubectl:

[ec2-user@ip-172-31-53-211 ~]$ curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.25.6/2023-01-30/bin/linux/amd64/kubectl
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 42.9M  100 42.9M    0     0  4860k      0  0:00:09  0:00:09 --:--:-- 5293k

[ec2-user@ip-172-31-53-211 ~]$ chmod +x ./kubectl

[ec2-user@ip-172-31-53-211 ~]$ mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$PATH:$HOME/bin


[ec2-user@ip-172-31-53-211 ~]$ kubectl version
Client Version: version.Info{Major:"1", Minor:"25+", GitVersion:"v1.25.6-eks-48e63af", GitCommit:"9f22d4ae876173884749c0701f01340879ab3f95", GitTreeState:"clean", BuildDate:"2023-01-24T19:22:49Z", GoVersion:"go1.19.5", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.7


6. ADD user credentials:

[ec2-user@ip-172-31-53-211 ~]$ aws configure
AWS Access Key ID [None]: A***************
AWS Secret Access Key [None]: J***********
Default region name [None]: us-east-1
Default output format [None]: json

[ec2-user@ip-172-31-53-211 ~]$ aws eks update-kubeconfig --region us-east-1 --name eks-test
Added new context arn:aws:eks:us-east-1:944153906994:cluster/eks-test to /home/ec2-user/.kube/config


[ec2-user@ip-172-31-53-211 ~]$ kubectl get svc


7- Create A Node Group For EKS Cluster With A New IAM Role:
AmazonEKS_CNI_Policy
AmazonEC2ContainerRegistryReadOnly
AmazonEKSWorkerNodePolicy 


kubectl config view

kubectl get nodes

kubectl get pods --all-namespaces



8- Clean Up:
- Delete The Node Group
- Delete The EKS Cluster
- Delete The IAM Role For EKS
- Delete The User
- Delete The IAM Role For Node Group
- Delete The EC2 Instance Used To Manage The EKS Cluster
- Delete The Security Group Created For The EC2 Instance
- Delete The Key Pair Created For The EC2 Instance

# Remove EKS Cluster entry from kubeconfig file
kubectl config unset clusters.[cluster-name]

# Remove EKS Context from kubeconfig file
kubectl config delete-context [context-name]
kubectl config unset contexts.[context-name]
# Remove EKS User entry from kubeconfig file  
kubectl config unset users.[username]



# If we wanto to switch to another cluster
kubectl config use-context [context-name]




